{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f5fd80-14b0-41da-8775-80afae4dcd9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Could not find target column among ['returned', 'is_returned', 'return', 'label', 'target']. Available: ['Order_ID', 'Product_ID', 'User_ID', 'Order_Date', 'Return_Date', 'Product_Category', 'Product_Price', 'Order_Quantity', 'Return_Reason', 'Return_Status', 'Days_to_Return', 'User_Age', 'User_Gender', 'User_Location', 'Payment_Method', 'Shipping_Method', 'Discount_Applied']",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Could not find target column among ['returned', 'is_returned', 'return', 'label', 'target']. Available: ['Order_ID', 'Product_ID', 'User_ID', 'Order_Date', 'Return_Date', 'Product_Category', 'Product_Price', 'Order_Quantity', 'Return_Reason', 'Return_Status', 'Days_to_Return', 'User_Age', 'User_Gender', 'User_Location', 'Payment_Method', 'Shipping_Method', 'Discount_Applied']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import argparse, json, yaml, joblib, warnings, sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- DEFAULT PATHS (edit if needed) ----------\n",
    "IN_CSV_DEFAULT = r\"C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\archive\\ecommerce_returns_synthetic_data.csv\"\n",
    "OUT_DIR_DEFAULT = r\"C:\\Users\\NXTWAVE\\Downloads\\Return Radar\"\n",
    "\n",
    "# Candidate columns for the target and common fields\n",
    "TARGET_CANDS = [\"returned\", \"is_returned\", \"return\", \"label\", \"target\"]\n",
    "ID_CANDS     = [\"order_id\", \"OrderID\", \"orderId\", \"id\", \"ID\"]\n",
    "CAT_CANDS    = [\"category\", \"Category\", \"product_category\", \"dept\", \"Department\"]\n",
    "BRAND_CANDS  = [\"brand\", \"Brand\"]\n",
    "DATE_CANDS   = [\"order_time\", \"order_date\", \"OrderDate\", \"purchase_date\"]\n",
    "TEXT_CANDS   = [\"review_text\", \"return_reason\", \"comments\", \"notes\", \"description\"]\n",
    "REGION_CANDS = [\"region\", \"Region\", \"state\", \"State\", \"city\", \"City\"]\n",
    "DEVICE_CANDS = [\"device\", \"platform\", \"channel\"]\n",
    "NUM_HINTS    = [\"price\", \"discount\", \"quantity\", \"qty\", \"delivered_days\",\n",
    "                \"promised_delivery_days\", \"shipping_cost\", \"margin\", \"weight\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def ensure_outdir(p: str | Path) -> Path:\n",
    "    p = Path(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def pick_col(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def detect_target(df: pd.DataFrame) -> str:\n",
    "    t = pick_col(df, TARGET_CANDS)\n",
    "    if not t:\n",
    "        raise SystemExit(f\"Could not find target column among {TARGET_CANDS}. Available: {df.columns.tolist()}\")\n",
    "    return t\n",
    "\n",
    "def coerce_binary(y: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map various encodings to {0,1}: 'Yes'/'No', 'y'/'n', 'true'/'false', strings '0'/'1', etc.\n",
    "    \"\"\"\n",
    "    y2 = y.copy()\n",
    "    if y2.dtype == bool:\n",
    "        return y2.astype(int)\n",
    "    # normalize case/strings\n",
    "    y2 = y2.astype(str).str.strip().str.lower()\n",
    "    mapping = {\n",
    "        \"1\":1, \"0\":0, \"yes\":1, \"no\":0, \"y\":1, \"n\":0, \"true\":1, \"false\":0, \"returned\":1, \"not returned\":0\n",
    "    }\n",
    "    return y2.map(lambda v: mapping.get(v, np.nan)).astype(float)\n",
    "\n",
    "def parse_possible_datetimes(df: pd.DataFrame, date_cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for col in date_cols:\n",
    "        if col in out.columns:\n",
    "            dt = pd.to_datetime(out[col], errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
    "            valid_frac = dt.notna().mean()\n",
    "            if valid_frac > 0.5:\n",
    "                out[f\"{col}_year\"]  = dt.dt.year\n",
    "                out[f\"{col}_month\"] = dt.dt.month\n",
    "                out[f\"{col}_dow\"]   = dt.dt.dayofweek\n",
    "                out[f\"{col}_hour\"]  = dt.dt.hour\n",
    "                # keep original column as categorical string (e.g., '2024-06' style) or drop it\n",
    "                # here we drop to avoid too many columns\n",
    "                out = out.drop(columns=[col])\n",
    "    return out\n",
    "\n",
    "def detect_feature_types(df: pd.DataFrame, target_col: str) -> Tuple[List[str], List[str], Optional[str]]:\n",
    "    id_col = pick_col(df, ID_CANDS)\n",
    "    drop = {target_col}\n",
    "    if id_col:\n",
    "        drop.add(id_col)\n",
    "    # Numeric vs categorical\n",
    "    num_cols = [c for c in df.columns\n",
    "                if pd.api.types.is_numeric_dtype(df[c]) and c not in drop]\n",
    "    cat_cols = [c for c in df.columns\n",
    "                if not pd.api.types.is_numeric_dtype(df[c]) and c not in drop]\n",
    "    return num_cols, cat_cols, id_col\n",
    "\n",
    "def build_pipeline(num_cols: List[str], cat_cols: List[str]) -> Pipeline:\n",
    "    num_proc = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),  # works with sparse output\n",
    "    ])\n",
    "    cat_proc = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True)),\n",
    "    ])\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", num_proc, num_cols),\n",
    "        (\"cat\", cat_proc, cat_cols),\n",
    "    ])\n",
    "    clf = LogisticRegression(\n",
    "        C=4.0, max_iter=500, class_weight=\"balanced\", n_jobs=-1, solver=\"lbfgs\"\n",
    "    )\n",
    "    return Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "\n",
    "def top_coef_for_positive(pipe: Pipeline, k: int = 20) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract top positive-class coefficients from LogisticRegression over transformed features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pre: ColumnTransformer = pipe.named_steps[\"pre\"]\n",
    "        clf: LogisticRegression = pipe.named_steps[\"clf\"]\n",
    "        names = pre.get_feature_names_out()\n",
    "        coefs = clf.coef_[0]\n",
    "        order = np.argsort(coefs)[::-1]\n",
    "        idx = order[:k]\n",
    "        return [(str(names[i]), float(coefs[i])) for i in idx]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# ---------- main training / export ----------\n",
    "def train_and_export(in_csv: Path, outdir: Path, seed: int = 42):\n",
    "    # Load\n",
    "    if not in_csv.exists():\n",
    "        raise SystemExit(f\"Input CSV not found: {in_csv}\")\n",
    "    df = pd.read_csv(in_csv)\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"Input CSV is empty.\")\n",
    "\n",
    "    # Target\n",
    "    target_col = detect_target(df)\n",
    "    y = coerce_binary(df[target_col])\n",
    "    # keep rows with known label (0/1)\n",
    "    mask = y.isin([0, 1])\n",
    "    if mask.sum() < 50:\n",
    "        raise SystemExit(f\"Too few labeled rows after target mapping: {mask.sum()} (need >= 50).\")\n",
    "    df = df.loc[mask].copy()\n",
    "    y = y.loc[mask].astype(int)\n",
    "\n",
    "    # Optional date parsing on likely date columns\n",
    "    candidates = [c for c in DATE_CANDS if c in df.columns]\n",
    "    df = parse_possible_datetimes(df, candidates)\n",
    "\n",
    "    # Identify features\n",
    "    num_cols, cat_cols, id_col = detect_feature_types(df, target_col)\n",
    "    # Drop target, and optional ID from features\n",
    "    X = df.drop(columns=[target_col] + ([id_col] if id_col else []))\n",
    "\n",
    "    # Train/valid split (stratified)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
    "\n",
    "    # Build & fit\n",
    "    pipe = build_pipeline(num_cols=[c for c in num_cols if c in X.columns],\n",
    "                          cat_cols=[c for c in cat_cols if c in X.columns])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "\n",
    "    # Evaluate\n",
    "    yhat = pipe.predict(Xte)\n",
    "    try:\n",
    "        yproba = pipe.predict_proba(Xte)[:, 1]\n",
    "        auc = float(roc_auc_score(yte, yproba))\n",
    "    except Exception:\n",
    "        yproba = None\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    rep = classification_report(yte, yhat, output_dict=True)\n",
    "    cm  = confusion_matrix(yte, yhat, labels=[0, 1])\n",
    "\n",
    "    # Save PKL model\n",
    "    model_path = outdir / \"returns_model.pkl\"\n",
    "    joblib.dump(pipe, model_path)\n",
    "\n",
    "    # Save HDF5 with holdout predictions\n",
    "    te_out = Xte.copy()\n",
    "    te_out[\"returned_true\"] = yte.values\n",
    "    te_out[\"returned_pred\"] = yhat\n",
    "    if yproba is not None:\n",
    "        te_out[\"proba_return\"] = yproba\n",
    "    # requires `tables` package installed\n",
    "    te_out.to_hdf(outdir / \"processed_returns.h5\", key=\"holdout\", mode=\"w\")\n",
    "\n",
    "    # Insights JSON (quick business views)\n",
    "    insights: Dict[str, object] = {\n",
    "        \"rows_total\": int(len(df)),\n",
    "        \"return_rate_total\": float(y.mean()),\n",
    "        \"auc\": auc,\n",
    "        \"top_positive_features\": top_coef_for_positive(pipe, k=20),\n",
    "    }\n",
    "    # Optional group by category / brand / region if columns exist\n",
    "    for name, cands in [(\"by_category\", CAT_CANDS), (\"by_brand\", BRAND_CANDS), (\"by_region\", REGION_CANDS)]:\n",
    "        col = pick_col(df, cands)\n",
    "        if col:\n",
    "            tmp = pd.DataFrame({col: df[col], \"returned\": y})\n",
    "            grp = tmp.groupby(col)[\"returned\"].mean().sort_values(ascending=False).head(20)\n",
    "            insights[name] = grp.round(4).to_dict()\n",
    "\n",
    "    with open(outdir / \"insights.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(insights, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # YAML metadata\n",
    "    meta = {\n",
    "        \"input_csv\": str(in_csv),\n",
    "        \"shape\": [int(df.shape[0]), int(df.shape[1])],\n",
    "        \"target\": target_col,\n",
    "        \"id_column\": id_col,\n",
    "        \"numeric_cols\": [c for c in num_cols if c in X.columns],\n",
    "        \"categorical_cols\": [c for c in cat_cols if c in X.columns],\n",
    "        \"model\": \"OneHot + LogisticRegression(class_weight=balanced, C=4.0, max_iter=500)\",\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": float(rep.get(\"accuracy\", 0.0)),\n",
    "            \"precision_pos\": float(rep.get(\"1\", {}).get(\"precision\", 0.0)),\n",
    "            \"recall_pos\": float(rep.get(\"1\", {}).get(\"recall\", 0.0)),\n",
    "            \"f1_pos\": float(rep.get(\"1\", {}).get(\"f1-score\", 0.0)),\n",
    "            \"precision_neg\": float(rep.get(\"0\", {}).get(\"precision\", 0.0)),\n",
    "            \"recall_neg\": float(rep.get(\"0\", {}).get(\"recall\", 0.0)),\n",
    "            \"f1_neg\": float(rep.get(\"0\", {}).get(\"f1-score\", 0.0)),\n",
    "            \"auc\": auc,\n",
    "        },\n",
    "    }\n",
    "    with open(outdir / \"build_metadata.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(meta, f, sort_keys=False)\n",
    "\n",
    "    # Also drop CSVs (optional but handy)\n",
    "    pd.DataFrame(rep).to_csv(outdir / \"eval_report.csv\")\n",
    "    pd.DataFrame(cm, index=[\"true_0\", \"true_1\"], columns=[\"pred_0\", \"pred_1\"]).to_csv(outdir / \"confusion_matrix.csv\")\n",
    "\n",
    "    print(\"\\n[OK] Artifacts written to:\", outdir)\n",
    "    for p in [\"processed_returns.h5\", \"returns_model.pkl\", \"build_metadata.yaml\", \"insights.json\",\n",
    "              \"eval_report.csv\", \"confusion_matrix.csv\"]:\n",
    "        print(\" \", outdir / p)\n",
    "\n",
    "# ---------- entry point ----------\n",
    "def main(argv: Optional[List[str]] = None):\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--data\", type=str, default=IN_CSV_DEFAULT, help=\"Path to ecommerce returns CSV\")\n",
    "    ap.add_argument(\"--outdir\", type=str, default=OUT_DIR_DEFAULT, help=\"Where to save artifacts\")\n",
    "    ap.add_argument(\"--seed\", type=int, default=42)\n",
    "    # Jupyter-safe:\n",
    "    args, _ = ap.parse_known_args(argv)\n",
    "\n",
    "    in_csv = Path(args.data)\n",
    "    outdir = ensure_outdir(args.outdir)\n",
    "    train_and_export(in_csv, outdir, seed=args.seed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf18e2-c3b6-4cf9-a43b-953970874f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
