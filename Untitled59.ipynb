{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8aad92-1a93-46d8-bb6b-60b4325bd3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Artifacts written to: C:\\Users\\NXTWAVE\\Downloads\\Return Radar\n",
      "  C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\processed_returns.h5\n",
      "  C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\returns_model.pkl\n",
      "  C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\build_metadata.yaml\n",
      "  C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\insights.json\n",
      "  C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\eval_report.csv\n",
      "  C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\confusion_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import argparse, json, yaml, joblib, warnings, sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------- Paths ---------\n",
    "IN_CSV_DEFAULT  = r\"C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\archive\\ecommerce_returns_synthetic_data.csv\"\n",
    "OUT_DIR_DEFAULT = r\"C:\\Users\\NXTWAVE\\Downloads\\Return Radar\"\n",
    "\n",
    "# --------- Helpers ---------\n",
    "def ensure_outdir(p: str | Path) -> Path:\n",
    "    p = Path(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def map_return_status(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Map Return_Status to {0,1}. Robust to common spellings.\"\"\"\n",
    "    x = s.astype(str).str.strip().str.lower()\n",
    "    pos = {\"returned\",\"return\",\"yes\",\"y\",\"true\",\"1\",\"approved\",\"accepted\",\"processed\"}\n",
    "    neg = {\"not returned\",\"not_returned\",\"no return\",\"no\",\"n\",\"false\",\"0\",\"rejected\",\"none\",\"nan\"}\n",
    "    def _m(v: str):\n",
    "        if v in pos: return 1\n",
    "        if v in neg: return 0\n",
    "        if \"not\" in v: return 0\n",
    "        if \"return\" in v: return 1\n",
    "        return np.nan\n",
    "    return x.map(_m)\n",
    "\n",
    "def derive_target(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Prefer Return_Status; fallback to presence of Return_Date.\"\"\"\n",
    "    y = None\n",
    "    if \"Return_Status\" in df.columns:\n",
    "        y = map_return_status(df[\"Return_Status\"])\n",
    "    if \"Return_Date\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"Return_Date\"], errors=\"coerce\")\n",
    "        y_date = dt.notna().astype(int)\n",
    "        y = y if y is not None else y_date\n",
    "        y = pd.Series(np.where(pd.isna(y), y_date, y), index=df.index)\n",
    "    if y is None:\n",
    "        raise SystemExit(\"Could not derive target: need Return_Status and/or Return_Date.\")\n",
    "    return y.astype(float)\n",
    "\n",
    "def parse_order_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"Order_Date\" in out.columns:\n",
    "        od = pd.to_datetime(out[\"Order_Date\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "        out[\"Order_Year\"]  = od.dt.year\n",
    "        out[\"Order_Month\"] = od.dt.month\n",
    "        out[\"Order_DOW\"]   = od.dt.dayofweek\n",
    "        out[\"Order_Hour\"]  = od.dt.hour\n",
    "        out = out.drop(columns=[\"Order_Date\"])\n",
    "    return out\n",
    "\n",
    "def detect_features(df: pd.DataFrame, target_col: str, drop_cols: List[str]) -> Tuple[List[str], List[str], Optional[str]]:\n",
    "    id_col = \"Order_ID\" if \"Order_ID\" in df.columns else None\n",
    "    drop = set(drop_cols + [target_col])\n",
    "    if id_col: drop.add(id_col)\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c not in drop]\n",
    "    cat_cols = [c for c in df.columns if not pd.api.types.is_numeric_dtype(df[c]) and c not in drop]\n",
    "    return num_cols, cat_cols, id_col\n",
    "\n",
    "def make_ohe_dense() -> OneHotEncoder:\n",
    "    \"\"\"Create a OneHotEncoder that returns DENSE output across sklearn versions.\"\"\"\n",
    "    try:\n",
    "        # sklearn >= 1.2\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        # sklearn < 1.2\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def build_pipeline(num_cols: List[str], cat_cols: List[str]) -> Pipeline:\n",
    "    num_proc = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        # Dense path is fine with with_mean=True, but keeping False is harmless\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "    cat_proc = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", make_ohe_dense()),\n",
    "    ])\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", num_proc, num_cols),\n",
    "        (\"cat\", cat_proc, cat_cols),\n",
    "    ])\n",
    "    # Dense features -> lbfgs works well\n",
    "    clf = LogisticRegression(C=4.0, max_iter=500, class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "    return Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "\n",
    "def top_positive_features(pipe: Pipeline, k: int = 20) -> List[Tuple[str, float]]:\n",
    "    try:\n",
    "        pre: ColumnTransformer = pipe.named_steps[\"pre\"]\n",
    "        clf: LogisticRegression = pipe.named_steps[\"clf\"]\n",
    "        names = pre.get_feature_names_out()\n",
    "        coefs = clf.coef_[0]\n",
    "        order = np.argsort(coefs)[::-1]\n",
    "        idx = order[:k]\n",
    "        return [(str(names[i]), float(coefs[i])) for i in idx]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# --------- Train & Export ---------\n",
    "def train_and_export(in_csv: Path, outdir: Path, seed: int = 42):\n",
    "    if not in_csv.exists():\n",
    "        raise SystemExit(f\"Input CSV not found: {in_csv}\")\n",
    "    df_raw = pd.read_csv(in_csv)\n",
    "    if df_raw.empty:\n",
    "        raise SystemExit(\"Input CSV is empty.\")\n",
    "\n",
    "    # Derive target\n",
    "    y = derive_target(df_raw)\n",
    "    mask = y.isin([0,1])\n",
    "    if mask.sum() < 50:\n",
    "        raise SystemExit(f\"Too few labeled rows after mapping Return_Status/Return_Date: {mask.sum()} (need >= 50).\")\n",
    "\n",
    "    df = df_raw.loc[mask].copy()\n",
    "    y = y.loc[mask].astype(int)\n",
    "\n",
    "    # Drop post-shipment leakage\n",
    "    leak_cols = []\n",
    "    if \"Return_Date\" in df.columns: leak_cols.append(\"Return_Date\")\n",
    "    if \"Return_Reason\" in df.columns: leak_cols.append(\"Return_Reason\")\n",
    "    if \"Days_to_Return\" in df.columns: leak_cols.append(\"Days_to_Return\")\n",
    "    target_col = \"__returned__\"\n",
    "    df[target_col] = y\n",
    "\n",
    "    # Parse order date into features\n",
    "    df = parse_order_date(df)\n",
    "\n",
    "    # Detect feature types\n",
    "    num_cols, cat_cols, id_col = detect_features(df, target_col, drop_cols=leak_cols)\n",
    "\n",
    "    # Build X,y\n",
    "    X = df.drop(columns=[target_col] + leak_cols + ([id_col] if id_col else []))\n",
    "    y = df[target_col].astype(int)\n",
    "\n",
    "    # Split\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
    "\n",
    "    # Pipeline (dense OHE)\n",
    "    pipe = build_pipeline(num_cols=[c for c in num_cols if c in X.columns],\n",
    "                          cat_cols=[c for c in cat_cols if c in X.columns])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "\n",
    "    # Evaluate\n",
    "    yhat = pipe.predict(Xte)\n",
    "    try:\n",
    "        yproba = pipe.predict_proba(Xte)[:, 1]\n",
    "        auc = float(roc_auc_score(yte, yproba))\n",
    "    except Exception:\n",
    "        yproba = None\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    rep = classification_report(yte, yhat, output_dict=True)\n",
    "    cm  = confusion_matrix(yte, yhat, labels=[0, 1])\n",
    "\n",
    "    # Save model\n",
    "    model_path = outdir / \"returns_model.pkl\"\n",
    "    joblib.dump(pipe, model_path)\n",
    "\n",
    "    # Save HDF5 with holdout predictions\n",
    "    te_out = Xte.copy()\n",
    "    te_out[\"returned_true\"] = yte.values\n",
    "    te_out[\"returned_pred\"] = yhat\n",
    "    if yproba is not None:\n",
    "        te_out[\"proba_return\"] = yproba\n",
    "    try:\n",
    "        te_out.to_hdf(outdir / \"processed_returns.h5\", key=\"holdout\", mode=\"w\")  # needs `tables`\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not write HDF5 (install `tables`):\", e)\n",
    "\n",
    "    # Insights JSON\n",
    "    insights: Dict[str, object] = {\n",
    "        \"rows_total\": int(len(df)),\n",
    "        \"return_rate_total\": float(y.mean()),\n",
    "        \"auc\": auc,\n",
    "        \"top_positive_features\": top_positive_features(pipe, 20),\n",
    "    }\n",
    "    for name, col in [(\"by_category\",\"Product_Category\"),\n",
    "                      (\"by_payment\",\"Payment_Method\"),\n",
    "                      (\"by_shipping\",\"Shipping_Method\"),\n",
    "                      (\"by_location\",\"User_Location\")]:\n",
    "        if col in df_raw.columns:\n",
    "            tmp = pd.DataFrame({col: df_raw.loc[mask, col], \"returned\": y})\n",
    "            grp = tmp.groupby(col)[\"returned\"].mean().sort_values(ascending=False).head(20)\n",
    "            insights[name] = grp.round(4).to_dict()\n",
    "\n",
    "    with open(outdir / \"insights.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(insights, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # YAML metadata\n",
    "    meta = {\n",
    "        \"input_csv\": str(in_csv),\n",
    "        \"shape\": [int(df.shape[0]), int(df.shape[1])],\n",
    "        \"target_from\": [\"Return_Status\", \"Return_Date (fallback)\"],\n",
    "        \"dropped_leak_columns\": leak_cols,\n",
    "        \"id_column\": id_col,\n",
    "        \"numeric_cols\": [c for c in num_cols if c in X.columns],\n",
    "        \"categorical_cols\": [c for c in cat_cols if c in X.columns],\n",
    "        \"model\": \"OneHot(dense) + LogisticRegression(class_weight=balanced, C=4.0, max_iter=500)\",\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": float(rep.get(\"accuracy\", 0.0)),\n",
    "            \"precision_pos\": float(rep.get(\"1\", {}).get(\"precision\", 0.0)),\n",
    "            \"recall_pos\": float(rep.get(\"1\", {}).get(\"recall\", 0.0)),\n",
    "            \"f1_pos\": float(rep.get(\"1\", {}).get(\"f1-score\", 0.0)),\n",
    "            \"precision_neg\": float(rep.get(\"0\", {}).get(\"precision\", 0.0)),\n",
    "            \"recall_neg\": float(rep.get(\"0\", {}).get(\"recall\", 0.0)),\n",
    "            \"f1_neg\": float(rep.get(\"0\", {}).get(\"f1-score\", 0.0)),\n",
    "            \"auc\": auc,\n",
    "        },\n",
    "    }\n",
    "    with open(outdir / \"build_metadata.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(meta, f, sort_keys=False)\n",
    "\n",
    "    # CSV helpers\n",
    "    pd.DataFrame(rep).to_csv(outdir / \"eval_report.csv\")\n",
    "    pd.DataFrame(cm, index=[\"true_0\",\"true_1\"], columns=[\"pred_0\",\"pred_1\"]).to_csv(outdir / \"confusion_matrix.csv\")\n",
    "\n",
    "    print(\"\\n[OK] Artifacts written to:\", outdir)\n",
    "    for p in [\"processed_returns.h5\",\"returns_model.pkl\",\"build_metadata.yaml\",\"insights.json\",\n",
    "              \"eval_report.csv\",\"confusion_matrix.csv\"]:\n",
    "        print(\" \", outdir / p)\n",
    "\n",
    "# --------- Entry Point ---------\n",
    "def main(argv: Optional[List[str]] = None):\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--data\", type=str, default=IN_CSV_DEFAULT)\n",
    "    ap.add_argument(\"--outdir\", type=str, default=OUT_DIR_DEFAULT)\n",
    "    ap.add_argument(\"--seed\", type=int, default=42)\n",
    "    # Jupyter-safe:\n",
    "    args, _ = ap.parse_known_args(argv)\n",
    "\n",
    "    in_csv = Path(args.data)\n",
    "    outdir = ensure_outdir(args.outdir)\n",
    "    train_and_export(in_csv, outdir, seed=args.seed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260edab-0db5-4b1d-a6dc-2371e7e207e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
