{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130944fa-ff5e-483f-8553-a42092c4e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_4560\\1509924530.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  od = pd.to_datetime(out[\"Order_Date\"], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Metrics saved: C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\prediction_report_ecommerce_returns_synthetic_data.csv C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\confusion_matrix_pred.csv\n",
      "[OK] Predictions saved: C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\predictions_ecommerce_returns_synthetic_data.csv\n",
      "\n",
      "Artifacts:\n",
      "  Predictions: C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\predictions_ecommerce_returns_synthetic_data.csv\n",
      "  Report     : C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\prediction_report_ecommerce_returns_synthetic_data.csv\n",
      "  CM         : C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\confusion_matrix_pred.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import argparse, sys\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# ----------- Paths (edit if needed) -----------\n",
    "MODEL_PATH_DEFAULT = Path(r\"C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\returns_model.pkl\")\n",
    "IN_CSV_DEFAULT     = Path(r\"C:\\Users\\NXTWAVE\\Downloads\\Return Radar\\archive\\ecommerce_returns_synthetic_data.csv\")\n",
    "OUT_DIR_DEFAULT    = Path(r\"C:\\Users\\NXTWAVE\\Downloads\\Return Radar\")\n",
    "\n",
    "# ----------- Helpers -----------\n",
    "def ensure_outdir(p: Path) -> Path:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def map_return_status(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Map Return_Status to {0,1}, robust to common spellings.\"\"\"\n",
    "    x = series.astype(str).str.strip().str.lower()\n",
    "    pos = {\"returned\",\"return\",\"yes\",\"y\",\"true\",\"1\",\"approved\",\"accepted\",\"processed\"}\n",
    "    neg = {\"not returned\",\"not_returned\",\"no return\",\"no\",\"n\",\"false\",\"0\",\"rejected\",\"none\",\"nan\"}\n",
    "    def _m(v: str):\n",
    "        if v in pos: return 1\n",
    "        if v in neg: return 0\n",
    "        if \"not\" in v: return 0\n",
    "        if \"return\" in v: return 1\n",
    "        return np.nan\n",
    "    return x.map(_m)\n",
    "\n",
    "def derive_ground_truth(df: pd.DataFrame) -> Optional[pd.Series]:\n",
    "    \"\"\"\n",
    "    Try to derive 0/1 ground truth from Return_Status and/or Return_Date.\n",
    "    Returns a Series of {0,1} or None if can’t be derived.\n",
    "    \"\"\"\n",
    "    y = None\n",
    "    if \"Return_Status\" in df.columns:\n",
    "        y = map_return_status(df[\"Return_Status\"])\n",
    "    if \"Return_Date\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"Return_Date\"], errors=\"coerce\")\n",
    "        y_date = dt.notna().astype(int)\n",
    "        y = y if y is not None else y_date\n",
    "        y = pd.Series(np.where(pd.isna(y), y_date, y), index=df.index)\n",
    "    if y is None:\n",
    "        return None\n",
    "    return y.astype(float).where(lambda s: s.isin([0,1]), other=np.nan).dropna()\n",
    "\n",
    "def parse_order_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add features derived from Order_Date (Year/Month/DOW/Hour) if present.\"\"\"\n",
    "    out = df.copy()\n",
    "    if \"Order_Date\" in out.columns:\n",
    "        od = pd.to_datetime(out[\"Order_Date\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "        out[\"Order_Year\"]  = od.dt.year\n",
    "        out[\"Order_Month\"] = od.dt.month\n",
    "        out[\"Order_DOW\"]   = od.dt.dayofweek\n",
    "        out[\"Order_Hour\"]  = od.dt.hour\n",
    "        # NOTE: during training we dropped Order_Date; do the same here\n",
    "        out = out.drop(columns=[\"Order_Date\"])\n",
    "    return out\n",
    "\n",
    "def drop_leakage_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop columns that were excluded during training because they leak post-shipment info.\n",
    "    \"\"\"\n",
    "    leak_cols = [c for c in [\"Return_Date\",\"Return_Reason\",\"Days_to_Return\"] if c in df.columns]\n",
    "    return df.drop(columns=leak_cols, errors=\"ignore\")\n",
    "\n",
    "def drop_id_and_target_like(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove ID/target-like columns that would not be used by the model.\n",
    "    (The pipeline saved in returns_model.pkl already knows its feature names,\n",
    "     so we keep inputs as close as possible.)\n",
    "    \"\"\"\n",
    "    cols_to_drop = [c for c in [\"Order_ID\", \"__returned__\", \"returned\", \"is_returned\", \"return\", \"label\", \"target\"]\n",
    "                    if c in df.columns]\n",
    "    return df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "def align_columns_for_pipeline(df: pd.DataFrame, model) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make sure df has the columns the pipeline expects.\n",
    "    If the training pipeline used ColumnTransformer with explicit column names,\n",
    "    sklearn will select by name. We therefore:\n",
    "      - parse Order_Date\n",
    "      - drop leakage/id/target-like cols\n",
    "      - keep all remaining columns; the ColumnTransformer will ignore unknowns\n",
    "        if it was built with explicit lists. (If it wasn’t, it still works\n",
    "        because it selects by the exact names used at fit time.)\n",
    "    \"\"\"\n",
    "    df2 = parse_order_date(df)\n",
    "    df2 = drop_leakage_cols(df2)\n",
    "    df2 = drop_id_and_target_like(df2)\n",
    "    return df2\n",
    "\n",
    "# ----------- Prediction core -----------\n",
    "def predict_file(model_path: Path, in_csv: Path, out_dir: Path) -> Tuple[Path, Optional[Path], Optional[Path]]:\n",
    "    if not model_path.exists():\n",
    "        raise SystemExit(f\"Model not found: {model_path}\\nRun your training script to create returns_model.pkl.\")\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    if not in_csv.exists():\n",
    "        raise SystemExit(f\"Input CSV not found: {in_csv}\")\n",
    "\n",
    "    # Read input; keep a copy for output\n",
    "    df_in = pd.read_csv(in_csv)\n",
    "    if df_in.empty:\n",
    "        raise SystemExit(\"Input CSV is empty.\")\n",
    "\n",
    "    # Prepare features to match training\n",
    "    X = align_columns_for_pipeline(df_in, model)\n",
    "\n",
    "    # Predict\n",
    "    try:\n",
    "        proba = model.predict_proba(X)[:, 1]\n",
    "        pred  = (proba >= 0.5).astype(int)\n",
    "    except Exception:\n",
    "        # If the classifier doesn’t expose proba\n",
    "        pred = model.predict(X)\n",
    "        proba = np.zeros(len(pred), dtype=float)\n",
    "\n",
    "    out = df_in.copy()\n",
    "    out[\"proba_return\"]  = proba\n",
    "    out[\"returned_pred\"] = pred\n",
    "\n",
    "    out_dir = ensure_outdir(out_dir)\n",
    "    base = in_csv.stem\n",
    "    pred_path = out_dir / f\"predictions_{base}.csv\"\n",
    "    out.to_csv(pred_path, index=False)\n",
    "\n",
    "    # If we can derive ground truth, compute quick metrics\n",
    "    y = derive_ground_truth(df_in)\n",
    "    report_path = None\n",
    "    cm_path = None\n",
    "\n",
    "    if y is not None and y.notna().any():\n",
    "        # Align with rows that had valid ground truth\n",
    "        mask = y.index\n",
    "        y_true = y.loc[mask].astype(int)\n",
    "        y_pred = pd.Series(pred, index=df_in.index).loc[mask].astype(int)\n",
    "\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "        rep = classification_report(y_true, y_pred, output_dict=True)\n",
    "        cm  = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "\n",
    "        # Save\n",
    "        rep_df = pd.DataFrame(rep)\n",
    "        report_path = out_dir / f\"prediction_report_{base}.csv\"\n",
    "        rep_df.to_csv(report_path)\n",
    "\n",
    "        cm_df = pd.DataFrame(cm, index=[\"true_0\",\"true_1\"], columns=[\"pred_0\",\"pred_1\"])\n",
    "        cm_path = out_dir / \"confusion_matrix_pred.csv\"\n",
    "        cm_df.to_csv(cm_path)\n",
    "\n",
    "        print(\"[OK] Metrics saved:\", report_path, cm_path)\n",
    "\n",
    "    print(\"[OK] Predictions saved:\", pred_path)\n",
    "    return pred_path, report_path, cm_path\n",
    "\n",
    "# ----------- CLI / Jupyter entry -----------\n",
    "def main(argv: Optional[List[str]] = None):\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--model\", type=str, default=str(MODEL_PATH_DEFAULT), help=\"Path to returns_model.pkl\")\n",
    "    ap.add_argument(\"--data\",  type=str, default=str(IN_CSV_DEFAULT),     help=\"Path to CSV to score\")\n",
    "    ap.add_argument(\"--outdir\",type=str, default=str(OUT_DIR_DEFAULT),    help=\"Where to write outputs\")\n",
    "    # Jupyter-safe parsing:\n",
    "    args, _ = ap.parse_known_args(argv)\n",
    "\n",
    "    pred_path, rep_path, cm_path = predict_file(Path(args.model), Path(args.data), Path(args.outdir))\n",
    "    print(\"\\nArtifacts:\")\n",
    "    print(\"  Predictions:\", pred_path)\n",
    "    if rep_path: print(\"  Report     :\", rep_path)\n",
    "    if cm_path:  print(\"  CM         :\", cm_path)\n",
    "\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6136f1-2026-48c0-905f-203835b3f813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
